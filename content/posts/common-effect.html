---
title: "Common Effects and Selection Bias"
author: "Michael Ore"
date: 2018-02-28
categories: []
tags: ["bayesianism", "R", "causality"]
---



<p>In the last post we looked at causal effects based on intermediate causes and common causes, both of which introduce a correlation that goes away when the “middle” variable is conditioned on. Now we’ll look at common effects which create correlations only when conditioned on.</p>
<p>For our first example, we’ll examine the relationship between SAT scores and high school GPA among college students. Suppose they have a latent “academic aptitude” variable for which SAT and GPA are imperfect measurements.</p>
<pre class="r"><code>library(tidyr)
library(bnlearn)
library(ggplot2)</code></pre>
<pre class="r"><code>set.seed(52)

# We&#39;ll simulate more data points than we&#39;ll display, to facilitate
# causal inference later
nStudent &lt;- 1000000
nDisplay &lt;- 1000

# The latent aptitude variable looks like IQ with a mean of 100 and
# standard deviation of 15, and can be thought of as like IQ. But it
# isn&#39;t intended to represent the result of an actual IQ test.
apt &lt;- rnorm(nStudent, 100, 15)
gpa &lt;- apt/34 + rnorm(nStudent, 0, 0.3)
sat &lt;- 11*apt + rnorm(nStudent, 0, 20)

df &lt;- data.frame(apt, gpa, sat)</code></pre>
<p>If we plot SAT and GPA, we see a clear positive correlation. This is a spurious correlation created by the common cause of latent aptitude. The relationship isn’t causal in the sense that if one were to hack into the College Board and change their official SAT score, this would have no bearing on their GPA. The correlation is nonetheless interesting, especially considering the latent aptitude can’t be measured directly.</p>
<pre class="r"><code>ggplot(df[1:nDisplay,], aes(gpa, sat)) +
  geom_point()</code></pre>
<p><img src="/posts/common-effect_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Now, let’s add a new variable representing which university each student is attending. We’ll use a simplified model for assigning students to unviversities. Universities evaluate applications by summing the percentiles of the SATs and GPAs, and admit a student if that amount is higher than some cutoff. There are nine universities, and each has their own cutoff. Students attend the most selective university they were admitted to. This cleanly seperates the students into distinct groups.</p>
<p>Each university studies the relationship between SAT and GPA for their own students. Let’s look at all of these studies on one plot.</p>
<pre class="r"><code># This creates a categorical variable that stratifies a continuous variable
# into discrete bins with equal probability. Contrast with the base function 
# &#39;cut&#39;, which creates bins of equal length.
stratify &lt;- function(vec, nStrats = 9) {
  return(factor(ceiling(nStrats*rank(vec)/length(vec))))
}</code></pre>
<pre class="r"><code>gpa.norm &lt;- (gpa-mean(gpa))/sd(gpa)
sat.norm &lt;- (sat-mean(sat))/sd(sat)
df$admit &lt;- gpa.norm + sat.norm + rnorm(nStudent, 0, 0.05)
df$admit.strat &lt;- stratify(df$admit)</code></pre>
<pre class="r"><code>ggplot(df[1:nDisplay,], aes(gpa, sat)) +
  geom_point(aes(color = admit.strat))</code></pre>
<p><img src="/posts/common-effect_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>This is the same data as before, but we’ve colored the points based on which university a student attends. The overall correlation is positive, but <em>within each university</em> the correlation is negative except in the top and bottom schools. Interesting.</p>
<p>This is another type of spurious correlation commonly known as <strong>selection bias</strong> or <strong>Berkson’s paradox</strong>. The university a student attends is a <strong>common effect</strong>, which is a sort of inverse of a common cause in the effect it has on our analyses.</p>
<p>A common effect starts out without introducing any spurious correlation. That would clearly be impossible in our example, as GPA and SAT are determined in high school before any student applies to any colleges.</p>
<p>The spurious correlation only appears when we condition on the common effect. In our example, it would appear because weakness in one admission criteria can be compensated for by strength in the other criteria.</p>
<p>Mind that selection bias doesn’t have to flip the direction of correlation. We can get any direction of correlation we want and even different correlations in different groups by abritrarily partitioning our data with our common effect. Here’s some extreme examples, just for fun.</p>
<pre class="r"><code>admit.l2 &lt;- sqrt(gpa.norm^2+sat.norm^2) + rnorm(nStudent, 0, 0.1)
df$admit.l2.strat &lt;- stratify(admit.l2)

ggplot(df[1:nDisplay,], aes(gpa, sat)) +
  geom_point(aes(color = admit.l2.strat))</code></pre>
<p><img src="/posts/common-effect_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>admit.cos &lt;- 2*cos(3*gpa.norm)+sat.norm + rnorm(nStudent, 0, 0.1)
df$admit.cos.strat &lt;- stratify(admit.cos)

ggplot(df[1:nDisplay,], aes(gpa, sat)) +
  geom_point(aes(color = admit.cos.strat))</code></pre>
<p><img src="/posts/common-effect_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>angles = 2*pi/5*(0:4)
mat &lt;- cbind(cos(angles)+sin(angles), sin(angles)-cos(angles))
vars &lt;- rbind(gpa.norm, sat.norm)
const &lt;- matrix(c(1/2, 1/2, 1/2, 1/2, 1/2), 5, nStudent)
halfplaneIx &lt;- as.numeric(mat %*% vars &lt; const)
shifter &lt;- matrix(0:4, 5, nStudent)
df$admit.partition &lt;- factor(colSums(matrix(bitwShiftL(halfplaneIx, shifter), 5, nStudent)))

ggplot(df[1:nDisplay,], aes(gpa, sat)) +
  geom_point(aes(color = admit.partition))</code></pre>
<p><img src="/posts/common-effect_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div id="learning-causal-structure-with-common-effects" class="section level2">
<h2>Learning causal structure with common effects</h2>
<p>Among intermediate causes, common causes, and common effects, common effects are special. They have the opposite effect from the other two, obervationally.</p>
<p>If Z is a common cause or intermediate cause for X and Y:
<span class="math display">\[X \perp\!\!\not\!\perp Y\;and\; X \perp\!\!\!\perp Y\,|\,Z\]</span>
If Z is a common effect for X and Y:
<span class="math display">\[X \perp\!\!\!\perp Y\;and\; X \perp\!\!\not\!\perp Y\,|\,Z\]</span></p>
<p>It’s for exactly this reason that we can distinguish them from data with <code>gs</code>.</p>
<pre class="r"><code>result = gs(df[,c(&#39;apt&#39;, &#39;gpa&#39;, &#39;admit&#39;, &#39;sat&#39;)])
plot(result)</code></pre>
<p><img src="/posts/common-effect_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>These dependencies and conditional dependencies are determined by <code>gs</code> with linear regression, as usual.</p>
</div>
<div id="inheritance-of-height" class="section level2">
<h2>Inheritance of height</h2>
<p>We’ll work through a subtler example too. This uses simulated data with parameters taken from a study<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> on inheritance of height in humans. X represents the father’s height, Y represents the mother’s height, and Z represents their child’s height in adulthood.</p>
<pre class="r"><code>mdl.height &lt;- model2network(&#39;[X][Y][Z|X:Y]&#39;)
X.dist &lt;- list(coef = c(&#39;(Intercept)&#39; = 67.68), sd = 2.7)
Y.dist &lt;- list(coef = c(&#39;(Intercept)&#39; = 62.48), sd = 2.39)
Z.dist &lt;- list(coef = c(&#39;(Intercept)&#39; = 14.08, X = 0.409, Y = 0.430), sd = 2.7)
fit.height &lt;- custom.fit(mdl.height, list(X = X.dist, Y = Y.dist, Z = Z.dist))</code></pre>
<pre class="r"><code>set.seed(200)
n &lt;- 10000
#
df.XYZ &lt;- rbn(fit.height, n)</code></pre>
<p>The father and mother’s heights are initially uncorrelated, with a small and statistically insignificant regression coefficient.</p>
<pre class="r"><code>summary(lm(Y ~ X, df.XYZ))</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ X, data = df.XYZ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6039 -1.5605 -0.0068  1.6012  9.2959 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 63.178924   0.602809   104.8   &lt;2e-16 ***
## X           -0.010682   0.008901    -1.2     0.23    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.388 on 9998 degrees of freedom
## Multiple R-squared:  0.000144,   Adjusted R-squared:  4.404e-05 
## F-statistic:  1.44 on 1 and 9998 DF,  p-value: 0.2301</code></pre>
<p>However the father and mother’s heights are correlated when <em>adjusting for the child’s height</em>, with a slightly larger but definitely statistically significant regression coefficient.</p>
<pre class="r"><code>summary(lm(Y ~ X + Z, df.XYZ))</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ X + Z, data = df.XYZ)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3351 -1.4928  0.0064  1.5213  8.3838 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 51.128395   0.649976   78.66   &lt;2e-16 ***
## X           -0.124081   0.008870  -13.99   &lt;2e-16 ***
## Z            0.287647   0.007687   37.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.236 on 9997 degrees of freedom
## Multiple R-squared:  0.123,  Adjusted R-squared:  0.1228 
## F-statistic:   701 on 2 and 9997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Adding variables to a regression never hurts predictive accuracy (at least in the given training data, neglecting overfitting). However, it can bias measurement of the true causal effect as illustrated by this effect.</p>
<p>Next, we’ll review methods of accurately measuring the magnitude of causal effects from observational data.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Clemons, Traci. “A look at the inheritance of height using regression toward the mean.” <em>Human biology</em> (2000): 447-454.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
