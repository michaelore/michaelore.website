---
title: "Causal Networks"
author: "Michael Ore"
date: 2018-01-25
categories: []
tags: ["bayesianism", "R", "causality"]
---



<p>As we discussed in the last post, Bayesian networks can often be written in multiple equivalent ways by reversing some of the arrows. Generally the most natural choice has arrows matching the causal direction of the effects. A Bayesian network in which the arrows have causal meaning is a <strong>causal network</strong>. In this post we’ll build up to a clearer idea of what that means.</p>
<p>Consider again the models A and B of IQ(X) vs log-income(Y) from the last post. They’re GBNs, so we can write them as systems of linear models as follows.</p>
<ol style="list-style-type: upper-alpha">
<li><p><span class="math display">\[
\begin{align*}
X &amp;= \beta_{10} + \epsilon_X \\
Y &amp;= \beta_{20} + \beta_{21}X + \epsilon_Y
\end{align*}
\]</span></p></li>
<li><p><span class="math display">\[
\begin{align*}
X &amp;= \beta_{10} + \beta_{11}Y + \epsilon_X \\
Y &amp;= \beta_{20} + \epsilon_Y
\end{align*}
\]</span></p></li>
</ol>
<p>Where <span class="math inline">\(\epsilon_X\)</span> and <span class="math inline">\(\epsilon_Y\)</span> are normally distributed error terms that represent any deviation between the linear models and the observed data, as in linear regression.</p>
<p>This form of models A and B is called a <strong>structural equation model</strong> (SEM). These are given a causal interpretation, where the variables on the right hand side (dependent variables) of an equation cause the variable on the left (independent variable). This causal information is not preserved with algebraic maniupulation, e.g. substituting the equation for X into Y’s equation in model A isn’t valid.</p>
<p>In the last post we had data generated by an opaque process and we fit our model parameters to the data. Now we’ll take another view by using our models to generate data. We can do this in R by creating a pipeline of random samples. For model A, for example, we’ll draw from X first and each of these draws lets us compute the correct mean for the normal distribution to draw Y from. We’ll call this sort of pipeline a <strong>data generating process</strong>. There’s nothing special about code, cause and effect processes in nature can also be thought of as data generating processes.</p>
<p>Let’s look at that code. We’re using the parameter values we learned from data in our last post.</p>
<pre class="r"><code># Set a RNG seed, so our simulations will give the same results every time we run our code
set.seed(2413)
# The number of samples we&#39;ll use for our simulations.
n &lt;- 1000
#
eps_X.A &lt;-rnorm(n, 0, 15) # Draw n random samples from a normal distribution with mean=0, standard deviation=15
eps_Y.A &lt;- rnorm(n, 0, 1)
X.A &lt;- 100 + eps_X.A
Y.A &lt;- 4 + X.A/60 + eps_Y.A
#
eps_X.B &lt;- rnorm(n, 0, sqrt(60^2/17))
eps_Y.B &lt;- rnorm(n, 0, sqrt(17/16))
Y.B &lt;- 17/3 + eps_Y.B
X.B &lt;- 80 + 60*Y.B/17 + eps_X.B
# simulations can also be done from a fitten BN, see ?rbn</code></pre>
<p>(<code>X.A</code>, <code>Y.A</code>) and (<code>X.B</code>, <code>Y.B</code>) have the same joint distribution, and the same distribution as the data generated directly from a MVN in the last post. Therefore, any predictions made based on observations will be the same (e.g. <span class="math inline">\(P_A(X|Y)=P_B(X|Y)\)</span>). When the arrows are given causal meaning, the models differ for predictions about <strong>interventions</strong>.</p>
<p>An <strong>intervention</strong> is the smallest possible change to a causal network that forces a variable to take a specific value. For a network structure, this means removing all arrows pointing at the intervened variable. For SEM, the target equation is replaced with a constant. For our simulation code, a random draw is replaced with a constant.</p>
<p>Interventions are written as e.g. <span class="math inline">\(\mathrm{do(X = x)}\)</span> The probability distribution for a variable given that another variable was intervened on is written e.g. <span class="math inline">\(P(Y\,|\,\mathrm{do}(X = x))\)</span>. Other equivalent notations are <span class="math inline">\(P(Y\,|\,\hat{x})\)</span> and <span class="math inline">\(P(Y_x)\)</span>, though we won’t use these.</p>
<p>Interventions generally change the distribution of a network. This change is not in general the same among observationally equivalent networks. e.g. in the case of our models A and B, <span class="math inline">\(P_A(Y\,|\,\mathrm{do}(X = x))\neq P_B(Y\,|\,\mathrm{do}(X = x))\)</span></p>
<p>This is what it looks like if we intervene with <span class="math inline">\(\mathrm{do}(Y = 20)\)</span> (e.g. an annual income of <span class="math inline">\(\$10^{20}\)</span>(!)).</p>
<pre class="r"><code>library(bnlearn)</code></pre>
<pre><code>## 
## Attaching package: &#39;bnlearn&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     sigma</code></pre>
<pre class="r"><code>mdl.A &lt;- model2network(&#39;[X][Y|X]&#39;)
mdl.B &lt;- model2network(&#39;[X|Y][Y]&#39;)

# &#39;mutilated&#39; also works for fitted BNs
mut.A &lt;- mutilated(mdl.A, list(Y = 20))
mut.B &lt;- mutilated(mdl.B, list(Y = 20))
mut.B$arcs &lt;- rbind(mut.B$arcs) # &#39;mutilated&#39; is broken when returning one edge, need to cast edges to matrix</code></pre>
<pre class="r"><code>par(mfrow=c(2, 2))
plot(mdl.A, main=&#39;A&#39;)
plot(mut.A, main=&#39;A | do(Y = 20)&#39;)
plot(mdl.B, main=&#39;B&#39;)
plot(mut.B, main=&#39;B | do(Y = 20)&#39;)</code></pre>
<p><img src="/posts/causal-networks_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>(A | do(Y = 20)) <span class="math display">\[
\begin{align*}
X &amp;= \beta_{10} + \epsilon_X \\
Y &amp;= 20
\end{align*}
\]</span></p>
<p>(B | do(Y = 20)) <span class="math display">\[
\begin{align*}
X &amp;= \beta_{10} + \beta_{11}Y + \epsilon_X \\
Y &amp;= 20
\end{align*}
\]</span></p>
<pre class="r"><code>set.seed(544)
#
eps_X.A.mut &lt;-rnorm(n, 0, 15)
eps_Y.A.mut &lt;- rnorm(n, 0, 1)
X.A.mut &lt;- 100 + eps_X.A.mut
Y.A.mut &lt;- rep(20, n)
#
eps_X.B.mut &lt;- rnorm(n, 0, sqrt(60^2/17))
eps_Y.B.mut &lt;- rnorm(n, 0, sqrt(17/16))
Y.B.mut &lt;- rep(20, n)
X.B.mut &lt;- 80 + 60*Y.B.mut/17 + eps_X.B.mut</code></pre>
<pre class="r"><code>mean(X.A.mut)</code></pre>
<pre><code>## [1] 99.26795</code></pre>
<pre class="r"><code>mean(X.B.mut)</code></pre>
<pre><code>## [1] 150.1895</code></pre>
<p>Expected IQ stays the same under model A, but increases by 50 points under model B. Model A more closely matches what we’d expect in reality.</p>
<p>We’ve kept things simple by sticking to two variables. In the next post we’ll examine causal interactions between three variables.</p>
